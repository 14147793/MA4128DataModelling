\documentclass[12pt]{article}

%opening
\usepackage{framed}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\begin{document}


\section*{Data Analysis Week 6 Quiz}
\begin{itemize}
\item Part 1 : Prediction Study Design
\item Part 2 : Cross Validation        
\item Part 3 : Predicting Regression        
\item Part 4 : Predicting Trees
\end{itemize}
\newpage
\subsection*{Binary Classification}
\subsubsection*{Defining true/false positives}
In general, Positive = identified and negative = rejected. Therefore:

\begin{itemize}
\item True positive = correctly identified

\item False positive = incorrectly identified

\item True negative = correctly rejected

\item False negative = incorrectly rejected
\end{itemize}
\subsubsection*{Medical testing example:}
\begin{itemize}
\item True positive = Sick people correctly diagnosed as sick

\item False positive= Healthy people incorrectly identified as sick

\item True negative = Healthy people correctly identified as healthy

\item False negative = Sick people incorrectly identified as healthy.
\end{itemize}
%-------------------------------------------------- %
\newpage
\subsection*{Steps in building a prediction}
\begin{enumerate}
\item Find the right data
\item Define your error rate
\item Split data into:
\begin{itemize}
\item Training Set
\item Testing Set
\item Validation Set(optional)
\end{itemize}
\item On the training set pick features
\item On the training set pick prediction function
\item On the training set cross-validate

\item If no validation - apply 1x to test set
\item If validation - apply to test set and refine
\item If validation - apply 1x to validation
\end{enumerate}
%-------------------------------------------------------%
\subsection*{Question 1}
Which of the following (pick one) is \textbf{NOT} a step in building a prediction model?
\subsubsection*{Options}
\begin{itemize}
\item[(i)] Defining the error rate			
\item[(ii)] Picking features			
\item[(iii)] Obtaining the right data			
%\item Selecting features with the test set.
\item[(iv)] Estimating test set accuracy with training-set accuracy. 
\item[(v)] (N0) Selecting features with the test set.
\end{itemize} 
\newpage
\subsection*{Cross Validation}
Bias Variance Trade-off \textit{http://scott.fortmann-roe.com/docs/BiasVariance.html}
\begin{itemize}
\item In a prediction problem, a model is usually given a dataset of known data 
on which training is run (\textit{training dataset}), and a dataset of unknown data (or \textit{first seen data/ testing dataset}) against which testing the model is performed.
\item Cross-validation is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. 
\item The goal of cross validation is to define a dataset to "test" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent data set (i.e., an unknown dataset, for instance from a real problem), etc.
\item Cross-validation is important in guarding against testing hypotheses suggested by the data (called "Type III errors"), especially where further samples 
are hazardous, costly or impossible to collect 
\end{itemize}
\subsubsection*{K-fold cross validation}
\begin{itemize}
\item In k-fold cross-validation, the original data set is randomly partitioned into $k$ equal size subsamples. 
\item Of the $k$ subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k - 1 subsamples are used as training data. 
\item The cross-validation process is then repeated k times (the folds), with each of the $k$ subsamples used exactly once as the validation data. \item The $k$ results from the folds can then be averaged (or otherwise combined) to produce a single estimation.
\item The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. 
\end{itemize}
\subsubsection*{Choosing K - Bias and Variance}
In general, when using k-fold cross validation, it seems to be the case that:
\begin{itemize}
\item A larger k will produce an estimate with smaller bias but potentially higher variance (on top of being computationally expensive)
\item A smaller k will lead to a smaller variance but may lead to a a biased estimate.
\end{itemize}

\subsubsection*{Leave-One-Out Cross-Validation}
\begin{itemize}
\item As the name suggests, leave-one-out cross-validation (LOOCV) involves using a single observation from the original sample as the validation data, and the remaining observations as the training data. 
\item This is repeated such that each observation in the sample is used once as the validation data. 
\item This is the same as a K-fold cross-validation with K being equal to the number of observations in the original sampling, i.e. \textbf{K=n}.
\end{itemize}

\newpage

%-------------------------------------------------------%
\subsection*{Question 2}
\begin{itemize}
\item \textbf{Part A} If K is small in a K-fold cross validation is the bias in the estimate of
out-of-sample (test set) accuracy smaller or bigger? 
\item \textbf{Part B} If K is small is the
variance in the estimate of out-of-sample (\textbf{test set}) accuracy smaller or bigger.
\item \textbf{Part C} Is K large or small in \textbf{leave one out} cross validation?
\end{itemize}
\subsubsection*{Options}

\begin{itemize}
\item[(i)]The bias is larger and the variance is smaller. Under leave one out cross validation K is equal to the sample size.			
\item[(ii)] (No) The bias is larger and the variance is smaller. Under leave one out cross validation K is \textbf{\textit{equal to one}}.			
\item[(iii)] (No) The bias is smaller and the variance is bigger. Under leave one out cross validation K is equal to one.
\item[(iv)] The bias is smaller and the variance is bigger. Under leave one out cross validation K is equal to the sample size,
%\item The bias is larger and the variance is smaller. Under leave one out cross validation K is equal to the sample size.
\end{itemize}
\newpage

%-------------------------------------------------------%
\subsection*{Logistic Regression}

Logistic regression is useful when you are predicting a binary outcome from a set of continuous predictor variables. 
\begin{framed}
\begin{verbatim}
# Logistic Regression
# where F is a binary factor and 
# x1-x3 are continuous predictors 

fit <- glm(F~x1+x2+x3,data=mydata,family=binomial())

summary(fit) # display results

confint(fit) # 95% CI for the coefficients

exp(coef(fit)) # exponentiated coefficients

exp(confint(fit)) # 95% CI for exponentiated coefficients

predict(fit, type="response") # predicted values

residuals(fit, type="deviance") # residuals

\end{verbatim}
\end{framed}
%-------------------------------------------------------%
\newpage
\subsection*{Type III Errors}

\begin{itemize}
\item Type III error is related to hypotheses suggested by the data, if tested using the data set that suggested them, are likely to be accepted even when they are not true. 

\item This is because circular reasoning would be involved: something seems true in the limited data set, therefore we hypothesize that it is true in general, therefore we (wrongly) test it on the same limited data set, which seems to confirm that it is true. 

\item Generating hypotheses based on data already observed, in the absence of testing them on new data, is referred to as post hoc theorizing.


\item The correct procedure is to test any hypothesis on a data set that was not used to generate the hypothesis.
\end{itemize}
\subsection*{Definitions}
\textbf{Accuracy Rate}\\
The accuracy rate calculates the proportion ofobservations being allocated to the \textbf{correct} group by the predictive model. It is calculated as follows:
\[ \frac{
\mbox{Number of Correct Classifications }}{\mbox{Total Number of Classifications }} \]

\[ = \frac{TP + TN}{TP+FP+TN+FN}\]


\noindent \textbf{Misclassification Rate}\\
The misclassification rate calculates the proportion ofobservations being allocated to the \textbf{incorrect} group by the predictive model. It is calculated as follows:
\[ \frac{
\mbox{Number of Incorrect Classifications }}{\mbox{Total Number of Classifications }} \]

\[ = \frac{FP + FN}{TP+FP+TN+FN}\]
Cross-Validation and Testing
==============================
In order to quantify the effects of bias and variance and construct the best possible estimator, we will split our training data into three parts: a training set, a cross-validation set, and a test set. As a general rule, the training set should be about 60% of the samples, and the cross-validation and test sets should be about 20% each.

The general idea is as follows. The model parameters (in our case, the coefficients of the polynomials) are learned using the training set as above. The error is evaluated on the cross-validation set, and the meta-parameters (in our case, the degree of the polynomial) are adjusted so that this cross-validation error is minimized. Finally, the labels are predicted for the test set. These labels are used to evaluate how well the algorithm can be expected to perform on unlabeled data.

Note Why do we need both a cross-validation set and a test set? Many machine learning practitioners use the same set of data as both a cross-validation set and a test set. This is not the best approach, for the same reasons we outlined above. Just as the parameters can be over-fit to the training data, the meta-parameters can be over-fit to the cross-validation data. For this reason, the minimal cross-validation error tends to under-estimate the error expected on a new set of data.


\subsection{Cross Validation}
%-------------------------------------------------------------------------------------%

The confusion
table is a table in which the rows are the observed categories of the dependent and
the columns are the predicted categories. When prediction is perfect all cases will lie on the
diagonal. The percentage of cases on the diagonal is the percentage of correct classifications. 
The cross validated set of data is a more honest presentation of the power of the
discriminant function than that provided by the original classifications and often produces
a poorer outcome. The cross validation is often termed a jack-knife classification, in that
it successively classifies \textbf{all cases but one} to develop a discriminant function and then
categorizes the case that was left out. This process is repeated with each case left out in
turn.This is known as leave-1-out cross validation. 

This cross validation produces a more reliable function. The argument behind it is that
one should not use the case you are trying to predict as part of the categorization process.


%-----------------------------------------------------------------------------------%
\subsection{Error Rates}

We can evaluate error rates by means of a training sample (to construct the discrimination rule) and a test sample.


An optimistic error rate is obtained by reclassifying the training data. (In the \textbf{\textit{training data}} sets, how many cases were misclassified). This is known as the \textbf{apparent error rate}.


The apparent error rate is obtained by using in the training set to estimate
the error rates. It can be severely optimistically biased, particularly for complex classifiers, and in the presence of over-fitted models.


If an independent test sample is used for classifying, we arrive at the  \textbf{true error rate}.The true error rate (or conditional error rate) of a classifier is the expected
probability of misclassifying a randomly selected pattern.
It is the error rate of an infinitely large test set drawn from the same distribution as the training data.



%---------------------------------------------------------------------------------------%

\subsection{Misclassification Cost}

As in all statistical procedures it is helpful to use diagnostic procedures to asses the efficacy of the discriminant analysis. We use \textbf{cross-validation} to assess the classification probability.
Typically you are going to have some prior rule as to what is an \textbf{acceptable misclassification rate}.

Those rules might involve things like, ``what is the cost of misclassification?" Consider a medical study where you might be able to diagnose cancer.

There are really two alternative costs. The cost of misclassifying someone as having cancer when they don't.
This could cause a certain amount of emotional grief. Additionally there would be the substantial cost of unnecessary treatment.

There is also the alternative cost of misclassifying someone as not having cancer when in fact they do have it.

A good classification procedure should
 \begin{itemize}
 \item result in few misclassifications
 \item take \textbf{\textit{prior probabilities of occurrence}} into account
 \item consider the cost of misclassification
 \end{itemize}
 
For example, suppose there tend to be more financially sound firms than bankrupt
firm. If we really believe that the prior probability of a financially
distressed and ultimately bankrupted firm is very small, then one should
classify a randomly selected firm as non-bankrupt unless the data
overwhelmingly favor bankruptcy.



There are two costs associated with discriminant analysis classification: The true misclassification cost per class, and the expected misclassification cost (ECM) per observation.

Suppose there we have a binary classification system, with two classes: class 1 and class 2.
Suppose that classifying a class 1 object as belonging to class 2 represents a more serious error than classifying a class 2 object as belonging to class 1. There would an assignable cost to each error.
$c(i|j)$ is the cost of classifying an observation into class $j$ if its true class is $i$.
The costs of misclassification can be defined by a cost matrix.

\begin{tabular}{|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  & Predicted & Predicted \\
   & Class 1 & Class 2 \\  \hline
  Class 1 & 0 & $c(2|1)$  \\
  Class 2 & $c(1|2)$ & 0 \\
  \hline
\end{tabular}




\subsection{Expected cost of misclassification (ECM)}
Let $p_1$ and $p_2$ be the prior probability of class 1 and class 2 respectively.
Necessarily $p_1$ + $p_2$ = 1.

The conditional probability of classifying an object as class 1 when it is in fact from
class 2 is denoted $p(1|2)$.
Similarly the conditional probability of classifying an object as class 2 when it is in
fact from class 1 is denoted $p(2|1)$.

\[ECM = c(2|1)p(2|1)p_1 + c(1|2)p(1|2)p_2\]
(In other words: the sum of the cost of misclassification times the (joint) probability of that misclassification.

A reasonable classification rule should have ECM as small as possible.







\end{document}
