\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{fancyhdr}
%\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 2011 13:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MA4128} \rhead{Mr. Kevin O'Brien}
\chead{Advanced Data Modelling}
%\input{tcilatex}


% http://www.norusis.com/pdf/SPC_v13.pdf
\begin{document}
\Large
\tableofcontents
\newpage
\section{ Week 5 General Theory Topics}


\subsection{Steps in Building a Predictive Model}
\begin{enumerate}
\item Find the right data
\item Define your error rate
\item Split data into:
\begin{itemize}
\item \textbf{Training Set}
\item \textbf{Testing Set}
\item \textbf{Validation Set} (optional)
\end{itemize}
\item On the training set select predictor variables (features)
\item On the training set generate your predictive model
\item On the training set cross-validate

%\item If no validation - apply 1x to test set
%\item If validation - apply to test set and refine
%\item If validation - apply 1x to validation
\end{enumerate}
%-------------------------------------------------------%
\subsection{Descriptive vs Predictive Models}

\begin{itemize}
	\item A \textbf{descriptive model} is only concerned with modeling the structure in the observed data. It makes sense to train and evaluate it on the same dataset.
	
	\item The \textbf{predictive model} is attempting a much more difficult problem, approximating the true discrimination function from a sample of data. We want to use algorithms that do not pick out and model all of the noise in our sample. We do want to chose algorithms that generalize beyond the observed data. It makes sense that we could only evaluate the ability of the model to generalize from a data sample on data that it had not see before during training.
	
	\item \textbf{IMPORTANT} The best descriptive model is accurate on the observed data. The best predictive model is accurate on unobserved data.
\end{itemize}


\subsection{Overfitting}
\begin{itemize}
	\item The flaw with evaluating a predictive model on training data is that it does not inform you on how well the model has generalized to new unseen data.
	
\item 	A model that is selected for its accuracy on the training dataset rather than its accuracy on an unseen test dataset is very likely have lower accuracy on an unseen test dataset. The reason is that the model is not as generalized. It has specalized to the structure in the training dataset. This is called overfitting.
\end{itemize}

\subsection{Cross-Validation and Testing}
%====================================================%
\begin{itemize}
	\item In order to build the best possible mode, we will split our training data into two parts: a training set and a test set. 
	
	\item 	The general idea is as follows. The model parameters (the regression coefficients) are learned using the training set as above. 
	\item The error is evaluated on the test set, and the meta-parameters are adjusted so that this cross-validation error is minimized. 

\end{itemize}	
\subsection{Cross Validation}
%-------------------------------------------------------------------------------------%
\begin{itemize}	
	\item 	The confusion
	table is a table in which the rows are the observed categories of the dependent and
	the columns are the predicted categories. When prediction is perfect all cases will lie on the
	diagonal. The percentage of cases on the diagonal is the percentage of correct classifications. 
	
	\item	The cross validated set of data is a more honest presentation of the power of the
	discriminant function than that provided by the original classifications and often produces
	a poorer outcome. 
	\item The cross validation is often termed a jack-knife classification, in that
	it successively classifies \textbf{all cases but one} to develop a discriminant function and then
	categorizes the case that was left out. This process is repeated with each case left out in
	turn.This is known as leave-1-out cross validation. 
	
	\item 	This cross validation produces a more reliable function. The argument behind it is that
	one should not use the case you are trying to predict as part of the categorization process.
\end{itemize}



%-----------------------------------------------------------------------------------%
\subsection{Error Rates}

\begin{itemize}
	\item We can evaluate error rates by means of a training sample (to construct build a model) and a test sample.
	
	
	\item 	An optimistic error rate is obtained by reclassifying the training data. (In the \textbf{\textit{training data}} sets, how many cases were misclassified). This is known as the \textbf{apparent error rate}.
	
	
	\item 	The apparent error rate is obtained by using in the training set to estimate
	the error rates. It can be severely optimistically biased, particularly for complex classifiers, and in the presence of over-fitted models.
	
	
	\item	If an independent test sample is used for classifying, we arrive at the  \textbf{true error rate}.The true error rate (or conditional error rate) of a classifier is the expected
	probability of misclassifying a randomly selected pattern.
	It is the error rate of an infinitely large test set drawn from the same distribution as the training data.
\end{itemize}




%---------------------------------------------------------------------------------------%




\subsection{Cross Validation}
\begin{itemize}
\item In a prediction problem, a model is usually given a dataset of known data 
on which training is run (\textit{training dataset}), and a dataset of unknown data (or \textit{first seen data/ testing dataset}) against which testing the model is performed.
\item Cross-validation is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice, with unseen data.
\item The goal of cross validation is to define a dataset to ``test" the model in the training phase, in order to limit problems like overfitting, give an insight on how the model will generalize to an independent data set (i.e., an unknown dataset, for instance from a real problem), etc.
\item Cross-validation is important in guarding against testing hypotheses suggested by the data (called ``\textbf{\textit{Type III errors}}"), especially where further samples 
are hazardous, costly or impossible to collect 
\end{itemize}
\subsection*{K-fold Cross Validation}
\begin{itemize}
\item In k-fold cross-validation, the original data set is randomly partitioned into $k$ equally sized subsamples (e.g. 10 samples).
 
\item Of the $k$ subsamples, a single subsample is retained as the testing data for testing the model, and the remaining k - 1 subsamples are used as training data. 
\item The cross-validation process is then repeated k times (the folds), with each of the $k$ subsamples used exactly once as the test data. \item The $k$ results from the folds can then be averaged (or otherwise combined) to produce a single estimation.
\item The advantage of this method over repeated random sub-sampling is that all observations are used for both training and testing, and each observation is used for testing exactly once. 
\end{itemize}
%\subsection*{Choosing K - Bias and Variance}
%In general, when using k-fold cross validation, it seems to be the case that:
%\begin{itemize}
%\item A larger k will produce an estimate with smaller bias but potentially higher variance (on top of being computationally expensive)
%\item A smaller k will lead to a smaller variance but may lead to a a biased estimate.
%\end{itemize}
\newpage

\subsection*{Leave-One-Out Cross-Validation}
\begin{itemize}
\item As the name suggests, \textbf{leave-one-out cross-validation}  \textbf{(LOOCV)} involves using a single observation from the original sample as the validation data, and the remaining observations as the training data. 
\item This is repeated such that each observation in the sample is used once as the validation data. 
\item This is the same as a K-fold cross-validation with K being equal to the number of observations in the original sampling, i.e. \textbf{K=n}.
\end{itemize}



%-------------------------------------------------------%

\subsection*{Type III Errors}

\begin{itemize}
\item Type III error is related to hypotheses suggested by the data, if tested using the data set that suggested them, are likely to be accepted even when they are not true. 

\item This is because circular reasoning would be involved: something seems true in the limited data set, therefore we hypothesize that it is true in general, therefore we (wrongly) test it on the same limited data set, which seems to confirm that it is true. 

\item Generating hypotheses based on data already observed, in the absence of testing them on new data, is referred to as \textbf{Post Hoc theorizing}.


\item The correct procedure is to test any hypothesis on a data set that was not used to generate the hypothesis.
\end{itemize}
%=================================================%
\newpage
\subsection{Binary Classification}
\noindent \textbf{Defining True/False Positives}
In general, Positive = identified and negative = rejected. Therefore:

\begin{itemize}
	\item True positive = correctly identified
	
	\item False positive = incorrectly identified
	
	\item True negative = correctly rejected
	
	\item False negative = incorrectly rejected
\end{itemize}
\subsubsection*{Medical Testing Example:}
\begin{itemize}
	\item True positive = Sick people correctly diagnosed as sick
	
	\item False positive= Healthy people incorrectly identified as sick
	
	\item True negative = Healthy people correctly identified as healthy
	
	\item False negative = Sick people incorrectly identified as healthy.
\end{itemize}
%-------------------------------------------------- %
\newpage
\subsection{Definitions (From Week 1)}
\textbf{Accuracy Rate}\\
The accuracy rate calculates the proportion ofobservations being allocated to the \textbf{correct} group by the predictive model. It is calculated as follows:
\[ \frac{
\mbox{Number of Correct Classifications }}{\mbox{Total Number of Classifications }} \]

\[ = \frac{TP + TN}{TP+FP+TN+FN}\]

\medskip

\noindent \textbf{Misclassification Rate}\\
The misclassification rate calculates the proportion ofobservations being allocated to the \textbf{incorrect} group by the predictive model. It is calculated as follows:
\[ \frac{
\mbox{Number of Incorrect Classifications }}{\mbox{Total Number of Classifications }} \]

\[ = \frac{FP + FN}{TP+FP+TN+FN}\]
\newpage

\subsection{Misclassification Cost}
\begin{itemize}
	\item As in all statistical procedures it is helpful to use diagnostic procedures to asses the efficacy of the discriminant analysis. We use \textbf{cross-validation} to assess the classification probability.
	Typically you are going to have some prior rule as to what is an \textbf{acceptable misclassification rate}.
	
	\item	Those rules might involve things like, \textit{``what is the cost of misclassification?"} Consider a medical study where you might be able to diagnose cancer.
	
	\item There are really two alternative costs. 
	\begin{itemize}
		\item[$\ast$] The cost of misclassifying someone as having cancer when they don't.
	This could cause a certain amount of emotional grief. Additionally there would be the substantial cost of unnecessary treatment.
	
	\item[$\ast$] There is also the alternative cost of misclassifying someone as not having cancer when in fact they do have it.
	\end{itemize}
	\item A good classification procedure should
	\begin{itemize}
		\item[$\ast$] result in few misclassifications
		\item[$\ast$] take \textbf{\textit{prior probabilities of occurrence}} into account
		\item[$\ast$] consider the cost of misclassification
	\end{itemize}
	
	\item 	For example, suppose there tend to be more financially sound firms than bankrupt
	firm. If we really believe that the prior probability of a financially
	distressed and ultimately bankrupted firm is very small, then one should
	classify a randomly selected firm as non-bankrupt unless the data
	overwhelmingly favor bankruptcy.
	
	
	
	\item 	There are two costs associated with discriminant analysis classification: The true misclassification cost per class, and the expected misclassification cost (ECM) per observation.
	
	\item 	\textbf{Example} Suppose there we have a binary classification system, with two classes: class 1 and class 2.
	Suppose that classifying a class 1 object as belonging to class 2 represents a more serious error than classifying a class 2 object as belonging to class 1. There would an assignable cost to each error.
	\textbf{c(i$|$j)} is the cost of classifying an observation into class $j$ if its true class is $i$.
	The costs of misclassification can be defined by a cost matrix.
	
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		& Predicted & Predicted \\
		& Class 1 & Class 2 \\  \hline
		Class 1 & 0 & $c(2|1)$  \\ \hline
		Class 2 & $c(1|2)$ & 0 \\
		\hline
	\end{tabular}
	\end{center}
	
\end{itemize}

\noindent \textbf{Expected cost of misclassification (ECM)}
\begin{itemize}
	\item Let $p_1$ and $p_2$ be the prior probability of class 1 and class 2 respectively.
	Necessarily $p_1$ + $p_2$ = 1.
	
\item	The conditional probability of classifying an object as class 1 when it is in fact from
	class 2 is denoted $p(1|2)$.
\item 	Similarly the conditional probability of classifying an object as class 2 when it is in
	fact from class 1 is denoted $p(2|1)$.
	
	\[ECM = c(2|1)p(2|1)p_1 + c(1|2)p(1|2)p_2\]
\textit{(In other words: the sum of the cost of misclassification times the (joint) probability of that misclassification.)}
	
\item 	A reasonable classification rule should have ECM as small as possible.
\end{itemize}







\end{document}
