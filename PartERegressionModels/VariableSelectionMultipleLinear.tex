\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode"CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 201113:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MA4128} \rhead{Kevin O'Brien} \chead{Week 9} %\input{tcilatex}

%http://www.electronics.dit.ie/staff/ysemenova/Opto2/CO_IntroLab.pdf
\begin{document}
	
	\tableofcontents
\section{Law of Parsimony}
Parsimonious: The simplest plausible model with the fewest possible number of variables.





\section{Variable Selection}
Often we require the optimal set of independent variables to adequately describe the data, without overfitting. As such we will use variable selection procedures. In statistical methods, the order in which the predictor variables are entered into (or taken out of) the model is determined according to the strength of their correlation with the response variable. Actually there are several versions of this method, called forward selection, backward selection and stepwise selection.

The main procedures are as follows:
\begin{itemize}
	\item Forward Selection
	\item Backward Elimination
	\item Stepwise Selection
\end{itemize}

The essential concept is the estimation of the relationship between a predictor
variable and a response variable after controlling for the effects of other
predictors in the equation. One such estimate is the semi-partial correlation coefficient.

\subsection{Variable Selection Procedures in SPSS}

\begin{itemize}
	\item \textbf{Enter}: This is the forced entry option. SPSS will enter at one time all specified variables regardless of significance levels.
	\item \textbf{Forward}: This method will enter variables one at a time, based on the significance value to enter.
	\item \textbf{Backward}: This enters all independent variables at one time and then removes variables one at a time based on a preset significance value to remove.
	\item \textbf{Stepwise}: This combines both forward and backward procedures. Since inter correlations are complex, the variance due to certain variables will change when new variables are entered into the equation. This is the most frequently used of the regression methods.
	\item \textbf{Remove}: This is the forced removal option. It requires an initial regression analysis usingthe Enter procedure. In the next block (Block 1 of 1) you may specify one or morevariables to remove. SPSS will then remove the specified variables and run the analysis again.
\end{itemize}

There are different ways that the relative contribution of each predictor variable can be assessed. In the ``simultaneous" method (which SPSS calls the Enter method), the researcher specifies the set of predictor variables that make up the model. The success of this model in predicting the criterion variable is then assessed.

%In contrast, ``hierarchical" methods enter the variables into the model in a specified order. The order specified should reflect some theoretical consideration or previous
%findings. If you have no reason to believe that one variable is likely to be more important than another you should not use this method. As each variable is entered into the model its contribution is assessed. If adding the variable does not significantly increase the predictive power of the model then the variable is
%dropped.

\subsection{Forward Selection}
We consider first forward Selection.In Forward selection, SPSS enters the variables into the model one at a time in an
order determined by the strength of their correlation with the response variable. The effect of adding each is assessed as it is entered, and variables that do not significantly add to the success of the model are excluded.

In this procedure, once a predictor is selected into the model, it cannot be removed. Other predictors may be added at future steps, but predictors already in the model remain in the model. As we will see, this is in contrast to SPSS's stepwise regression, in which we can specify criteria for both adding and removing predictors at each step.

\subsection{Steps in Forward Selection}
\textbf{Step 1}\\
Firstly, the predictor variable with the largest squared correlation with the dependent variable Y is entered into the model. Since this is the first step of the selection procedure, entering the predictor with the largest squared correlation is equivalent to entering the predictor with the \textbf{largest squared semi-partial} correlation. 

It may seem trivial to bring up the idea of semi-partial correlation at step 1 of the procedure, but we do so because at subsequent steps, the criterion for entrance into the regression equation will be the squared semi-partial correlation (or equivalently, the amount of variance contributed by the new predictor over and above variables already entered into the equation).

\textbf{Step 2}\\
The unselected predictor variable with the largest squared semi-partial correlation with the dependent variable (hence referred to as $Y$) is selected. That is, the predictor with the largest correlation with $Y$ after being adjusted for the first predictor, is entered if it meets entrance criteria in terms of preset statistical significance for entry, what SPSS refers to as
\textbf{\texttt{PIN}} (probability of entry) criteria. 
It is important to note that even once this new predictor is entered at step 2, the predictor entered at step 1 remains in the equation, even if it's new semi-partial correlation with $Y$ is now less than what it was at step 1. 

This is the nature of the forward selection procedure, it does not re-evaluate already-entered predictors into the model after adding new variables. That is, it only add predictors to the model. 

%Again, this is in contrast to SPSS’s stepwise procedure (to be discussed in some detail shortly) in which in addition to entrance criteria being specified for new variables, removal criteria is also specified at each stage of the variable-selection procedure.
\textbf{Step 3}\\
The next unselected predictor with the largest squared semi-partial correlation with $Y$ is then selected. That is, the predictor with the largest correlation with Y after being adjusted for both of the first two predictors is entered. 

Selection for entrance of this variable is conditional upon its relationship with the previously entered variables at step 1 and step 2. Hence, for a variable to be entered at step 3, SPSS asks the question, \textit{"Which among available variables currently not entered into the regression equation contribute most to variance explained in Y given that variables entered at steps 1 and 2 remain in the model?"} Translated into statistical language, what this question boils down to is selecting the variable that has the largest statistically significant squared semi-partial correlation with Y.

\textbf{Subsequent Steps } \\
We do not detail subsequent steps for the reason that they mimic the preceding steps. It is worth noting that we didn't even really need to detail steps 2 and 3, and could have just stated the "rule" of forward regression by referring to the first step alone. 

We can state the general rule of forward regression as follows:
\begin{quote}
	Forward regression, at each step of the selection procedure from step 1 through subsequent steps, chooses the predictor variable with the greatest squared semi-partial correlation with the response variable for entry into the regression equation. The given predictor will be entered if it satisfies entrance criteria (significance level, PIN) specified in advance by the researcher.
\end{quote}

The above is the simplest way to describe the procedural routine of how forward regression operates. What is perhaps most noteworthy about the above rule is what is not included just as much as what is included in the statement. Notice that nowhere in the rule is there any mention of removal of predictors at any step of the selection process. 

\subsection{Backward Selection}
In Backward selection, SPSS enters all the predictor variables into the model. The weakest predictor variable is then removed and the regression re-calculated. If this significantly weakens the model then the predictor variable is re-entered, otherwise it is deleted. This procedure is then repeated until only useful predictor variables remain in the model.

\subsection{Stepwise Selection}
Stepwise is the most sophisticated of these statistical methods. Each variable is entered in sequence and its value assessed. If adding the variable contributes to the model then it is retained, but all other variables in the model are then re-tested to see if they are still contributing to the success of the model. If they no longer contribute significantly they are removed. Thus, this method should ensure that you end up with the smallest possible set of predictor variables included in your model.

\subsection{The Remove Option}
In addition to the Enter, Stepwise, Forward and Backward methods, SPSS also offers the Remove method in which variables are removed from the model in a block - the use of this method will not be described here.

If you have no theoretical model in mind, and/or you have relatively low numbers
of cases, then it is probably safest to use Enter, the simultaneous method. Statistical
procedures should be used with caution and only when you have a large number of
cases. This is because minor variations in the data due to sampling errors can have a
large effect on the order in which variables are entered and therefore the likelihood
of them being retained. However, one advantage of the Stepwise method is that it
should always result in the most parsimonious model. This could be important if
you wanted to know the minimum number of variables you would need to measure
to predict the dependent variable. If for this, or some other reason, you decide to
select a statistical method, then you should really attempt to validate your results
with a second independent set of data. The can be done either by conducting a
second study, or by randomly splitting your data set into two halves . Only results that are common to both analyses should be reported.

\subsection{F values}

At each step, SPSS performs the following calculations: for each variable currently in the model, it computes the t-statistic for its estimated coefficient, squares it, and reports this as its \textbf{F-to-remove} statistic; for each variable not in the model, it computes the t-statistic that its coefficient would have if it were the next variable added, squares it, and reports this as its \textbf{F-to-enter} statistic.

At the next step, the program automatically enters the variable with the highest F-to-enter statistic, or removes the variable with the lowest F-to-remove statistic, in accordance with certain specified values. 

(Important: F = t-squared)

\subsection{Stepping Method Criteria}
Stepwise methods include or remove one independent variable at each step, based (by default) on the probability of F (p-value). The limits for the criteria controlling variable inclusion or removal can be specified by defining probabilities for \textbf{F-to-enter/F-to-remove} (or otherwise \textbf{values of F-to-enter/F-to-remove}, not recommended without a very thorough understanding of the F-distribution).




The following three stepwise methods are available.

\begin{itemize}
	\item Stepwise Based on the p-value of F (probability of F), SPSS starts by entering the variable with the smallest p-value; at the next step again the variable (from the list of variables not yet in the equation) with the smallest p-value for F and so on. 
	
	Variables already in the equation are removed if their p-value becomes larger than the default limit due to the inclusion of another variable. The method terminates when no more variables are eligible for inclusion or removal.	
	
	This methods is based on both probability-to-enter (PIN) and probability to remove (POUT).
	\item Backward Elimination: First all variables are entered into the equation and then sequentially removed. For each step SPSS provides statistics, namely $R^2$. At each step, the largest probability of F is removed (if the value is larger than POUT.
	\item Forward Forward selection: at each step the variable not yet in the equation with the smallest probability of F is entered. as long as the value is smaller than PIN. The procedure stops when there are no variables that meet the entry criterion.
\end{itemize}
\newpage


\section{Variable Selection Procedures}

\begin{itemize}
	\item Enter: This is the forced entry option. SPSS will enter at one time all specified variables regardless of significance levels.
	\item Forward: This method will enter variables one at a time, based on the significance value to enter.
	\item Backward: This enters all independent variables at one time and then removes variables one at a time based on a preset significance value to remove.
	\item Stepwise: This combines both forward and backward procedures. Since inter correlations are complex, the variance due to certain variables will change when new variables are entered into the equation. This is the most frequently used of the regression methods.
	\item Remove: This is the forced removal option. It requires an initial regression analysis usingthe Enter procedure. In the next block (Block 1 of 1) you may specify one or morevariables to remove. SPSS will then remove the specified variables and run the analysis again.
\end{itemize}
There are different ways that the relative contribution of each predictor variable can be assessed. In the “simultaneous” method (which SPSS calls the Enter method), the researcher specifies the set of predictor variables that make up the model. The success of this model in predicting the criterion variable is then assessed.

In contrast, “hierarchical” methods enter the variables into the model in a specified order. The order specified should reflect some theoretical consideration or previous
findings. If you have no reason to believe that one variable is likely to be more important than another you should not use this method. As each variable is entered into the model its contribution is assessed. If adding the variable does not significantly increase the predictive power of the model then the variable is
dropped.

In “statistical” methods, the order in which the predictor variables are entered into (or taken out of) the model is determined according to the strength of their correlation with the criterion variable. Actually there are several versions of this method, called forward selection, backward selection and stepwise selection.

\subsection{Forward Selection}
In Forward selection, SPSS enters the variables into the model one at a time in an
order determined by the strength of their correlation with the criterion variable. The effect of adding each is assessed as it is entered, and variables that do not significantly add to the success of the model are excluded.

\subsection{Forward Regression}
We consider first SPSS’s forward regression. In this procedure, once a predictor is selected into the model, it cannot be removed. Other predictors may be added at future steps, but predictors already in the model remain in the model. As we will see, this is in contrast to SPSS’s stepwise regression, in which we can specify criteria for both adding and removing predictors at each step.
We detail now a procedural description of SPSS’s forward selection procedure.

\textbf{Step 1}\\
The predictor with the largest squared correlation with Y is entered into the model. Since this is the first step of the selection procedure, entering the predictor with the largest squared correlation is equivalent to entering the predictor with the largest squared semipartial correlation. It may seem trivial to bring up the idea of semipartial correlation at step 1 of the procedure, but we do so because at subsequent steps, the criterion for entrance into the regression equation will be the squared semipartial correlation (or equivalently, the amount of variance contributed by the new predictor over and above variables already entered into the equation).

\textbf{Step 2}\\
The predictor with the largest squared semipartial correlation with Y is selected. That is, the predictor with the largest correlation with Y after being adjusted for the first predictor, is entered if it meets entrance criteria in terms of preset statistical significance for entry, what SPSS refers to as “PIN” (probability of entry, or “in”) criteria. Be sure to note that even once this new predictor is entered at step 2, the predictor entered at step 1 remains in the equation, even if it’s new semipartial correlation with Y is now less than what it was at step 1. This is the nature of the forward selection procedure, it does not re-evaluate already-entered predictors into the model after adding new variables. That is, it only add predictors to the model. Again, this is in contrast to SPSS’s stepwise procedure (to be discussed in some detail shortly) in which in addition to entrance criteria being specified for new variables, removal criteria is also specified at each stage of the variable-selection procedure.
\textbf{Step 3}\\
The predictor with the largest squared semipartial correlation with Y is selected. That is, the predictor with the largest correlation with Y after being adjusted for both of the first predictors is entered. Be sure to note that the entrance of this variable is conditional upon its relationship with the previously entered variables at step 1 and step 2. Hence, for a variable to be entered at step 3, SPSS asks the question, “Which among available variables currently not entered into the regression equation contribute most to variance explained in Y given that variables entered at steps 1 and 2 remain in the model?” Translated into statistical language, what this question boils down to is selecting the variable that has the largest statistically significant squared semipartial correlation with Y.

\textbf{Steps 4, 5, 6, } \\
We do not detail subsequent steps for the reason that they mimic the preceding steps. It is worth noting that we didn’t even really need to detail steps 2 and 3, and could have just stated the “rule” of forward regression by referring to the first step alone. We can state the general rule of forward regression as follows:
Forward regression, at each step of the selection procedure from step 1 through subsequent steps, chooses the predictor variable with the greatest squared semipartial correlation with the response variable for entry into the regression equation. The given predictor will be entered if it satisfies entrance criteria (significance level, PIN) specified in advance by the researcher.

The above is the simplest way to describe the procedural routine of how forward regression operates. What is perhaps most noteworthy about the above rule is what is not included just as much as what is included in the statement. Notice that nowhere in the rule is there any mention of removal of predictors at any step of the selection process. Forward selection only

\subsection{Backward Selection}
In Backward selection, SPSS enters all the predictor variables into the model. The weakest predictor variable is then removed and the regression re-calculated. If this
significantly weakens the model then the predictor variable is re-entered – otherwise it is deleted. This procedure is then repeated until only useful predictor variables remain in the model.

\subsection{Stepwise Selection}
Stepwise is the most sophisticated of these statistical methods. Each variable is entered in sequence and its value assessed. If adding the variable contributes to the model then it is retained, but all other variables in the model are then re-tested to see if they are still contributing to the success of the model. If they no longer contribute significantly they are removed. Thus, this method should ensure that you end up with the smallest possible set of predictor variables included in your model.


In addition to the Enter, Stepwise, Forward and Backward methods, SPSS also offers the Remove method in which variables are removed from the model in a block – the use of this method will not be described here.

If you have no theoretical model in mind, and/or you have relatively low numbers
of cases, then it is probably safest to use Enter, the simultaneous method. Statistical
procedures should be used with caution and only when you have a large number of
cases. This is because minor variations in the data due to sampling errors can have a
large effect on the order in which variables are entered and therefore the likelihood
of them being retained. However, one advantage of the Stepwise method is that it
should always result in the most parsimonious model. This could be important if
you wanted to know the minimum number of variables you would need to measure
to predict the criterion variable. If for this, or some other reason, you decide to
select a statistical method, then you should really attempt to validate your results
with a second independent set of data. The can be done either by conducting a
second study, or by randomly splitting your data set into two halves . Only results that are common to both analyses should be reported.

\subsection{Stepwise Regression}
Stepwise regression combines forward selection and backward elimination. At each
step, the best remaining variable is added, provided it passes the significant at 5%
criterion, then all variables currently in the regression are checked to see if any can be
removed, using the greater than 10\% significance criterion. The process continues
until no more variables are added or removed. This is the one we shall use. It is not
guaranteed to find the best subset of independents but it will find a subset close to the
best.
%
%\subsection{Checking prediction success: using training and validation sets}
%Another approach is to randomly assign your cases to two datasets. The first, called
%the training set, is used to calculate the regression.
%
%The second is called the validation set. The predicted score is calculated for all the cases in the validation set, but as we
%already have their actual scores, we can find the residuals and their standard
%deviation.

%%--------------------------------------------------------------------------- %
%\newpage
%
%
%
%%------------------------------------------------------------%
%\subsection{Adjusted R square}
%
%In a multiple linear regression model, adjusted R square measures the proportion of the variation in the dependent variable
%accounted for by the independent variables.


%------------------------------------------------------------%

%\subsection{Model Selection}
%
%Model selection is the task of selecting a statistical model from a set of potential models, given data.


%------------------------------------------------------------%









\newpage
\section{Stepwise Regression using \texttt{R}}

SPSS can be very opaque in determining how particularly statistical routines are carried out. Conversely the statistical programming language \texttt{R} is usually quite clear, once a familiarity with the language has been developed.

For variable selection procedures, \texttt{R} used the AIC criterion. When comparing multiple candidate models, the candidate model with the lowest AIC value is the best model. We will use \texttt{R} output to revise variable selection procedures. Recall that we used the \textbf{\textit{mtcars}} data set. The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (197374 models). For this data, we tried to determine the optimal set of independent variables to predict the dependent variables \textbf{\textit{mpg}} (miles per gallon).

\begin{description}
	\item[cyl]  Number of cylinders
	\item[disp]	 Displacement (cu.in.)
	\item[hp]  Gross horsepower
	\item[drat]	 Rear axle ratio
	\item[wt] Weight (lb/1000)
	\item[qsec]	 1/4 mile time
	\item[vs] V/S
	\item[am] Transmission (0 = automatic, 1 = manual)
	\item[gear]	 Number of forward gears
	\item[carb]	  Number of carburetors
\end{description}

\subsection{Backward Elimination}
The initial model contains all of the independent variables. Candidate models, whereby each of the independent variables are individually removed from the model are fitted. The AIC value for each reduced model is computed. The unreduced model is also used for comparison. The AIC values are tabulated to determine which removal results in the lowest AIC value. In this first case, the removal of cyl would reduced the AIC value from 70.898 (see bottom row) to 68.915. Thus the independent variable cyl is removed from the set of IVs.
\begin{framed}
	\begin{verbatim}
	Start:  AIC=70.9
	mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb
	
	Df Sum of Sq    RSS    AIC
	- cyl   1    0.0799 147.57 68.915
	- vs    1    0.1601 147.66 68.932
	- carb  1    0.4067 147.90 68.986
	- gear  1    1.3531 148.85 69.190
	- drat  1    1.6270 149.12 69.249
	- disp  1    3.9167 151.41 69.736
	- hp    1    6.8399 154.33 70.348
	- qsec  1    8.8641 156.36 70.765
	<none>              147.49 70.898
	- am    1   10.5467 158.04 71.108
	- wt    1   27.0144 174.51 74.280
	\end{verbatim}
\end{framed}
In the second phase, the process is repeated. This time removing vs results in an AIC value of 66.973. It is then removed from the set of IVs. For this phase, the unreduced model is the model fitted by all independent variables except cyl, which was removed in the previous phase.
\begin{framed}
	\begin{verbatim}
	Step:  AIC=68.92
	mpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb
	
	Df Sum of Sq    RSS    AIC
	- vs    1    0.2685 147.84 66.973
	- carb  1    0.5201 148.09 67.028
	- gear  1    1.8211 149.40 67.308
	- drat  1    1.9826 149.56 67.342
	- disp  1    3.9009 151.47 67.750
	- hp    1    7.3632 154.94 68.473
	<none>              147.57 68.915
	- qsec  1   10.0933 157.67 69.032
	- am    1   11.8359 159.41 69.384
	- wt    1   27.0280 174.60 72.297
	
	\end{verbatim}
\end{framed}
this process continues until the removal of an IV will not results in an improvement in AIC. This is indicated by having the $<none>$ ( i.e unreduced model) having the lowest AIC value. At the end of the output is the optimal model, according to the backward elimination procedure, using the IVs : am , qsec and wt.
\begin{framed}
	\begin{verbatim}
	
	
	Step:  AIC=61.31
	mpg ~ wt + qsec + am
	
	Df Sum of Sq    RSS    AIC
	<none>              169.29 61.307
	- am    1    26.178 195.46 63.908
	- qsec  1   109.034 278.32 75.217
	- wt    1   183.347 352.63 82.790
	
	Call:
	lm(formula = mpg ~ wt + qsec + am)
	
	Coefficients:
	(Intercept)           wt         qsec           am
	9.618       -3.917        1.226        2.936
	\end{verbatim}
\end{framed}
\subsection{Stepwise Regression}
Stepwise Regression differs from Backward Elimination, in that it allows IVs to be re-introduced. Hence the $+$ signs from the second phase onwards.
\begin{framed}
	\begin{verbatim}
	mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb
	
	Df Sum of Sq    RSS    AIC
	- cyl   1    0.0799 147.57 68.915
	- vs    1    0.1601 147.66 68.932
	- carb  1    0.4067 147.90 68.986
	- gear  1    1.3531 148.85 69.190
	- drat  1    1.6270 149.12 69.249
	- disp  1    3.9167 151.41 69.736
	- hp    1    6.8399 154.33 70.348
	- qsec  1    8.8641 156.36 70.765
	<none>              147.49 70.898
	- am    1   10.5467 158.04 71.108
	- wt    1   27.0144 174.51 74.280
	
	Step:  AIC=68.92
	mpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb
	
	Df Sum of Sq    RSS    AIC
	- vs    1    0.2685 147.84 66.973
	- carb  1    0.5201 148.09 67.028
	- gear  1    1.8211 149.40 67.308
	- drat  1    1.9826 149.56 67.342
	- disp  1    3.9009 151.47 67.750
	- hp    1    7.3632 154.94 68.473
	<none>              147.57 68.915
	- qsec  1   10.0933 157.67 69.032
	- am    1   11.8359 159.41 69.384
	+ cyl   1    0.0799 147.49 70.898
	- wt    1   27.0280 174.60 72.297
	
	
	\end{verbatim}
\end{framed}
Again, the procedure finishes when it is found that the unchanged model has the lowest of all possible AIC values.
\begin{framed}
	\begin{verbatim}
	Step:  AIC=61.31
	mpg ~ wt + qsec + am
	
	Df Sum of Sq    RSS    AIC
	<none>              169.29 61.307
	+ hp    1     9.219 160.07 61.515
	+ carb  1     8.036 161.25 61.751
	+ disp  1     3.276 166.01 62.682
	+ cyl   1     1.501 167.78 63.022
	+ drat  1     1.400 167.89 63.042
	+ gear  1     0.123 169.16 63.284
	+ vs    1     0.000 169.29 63.307
	- am    1    26.178 195.46 63.908
	- qsec  1   109.034 278.32 75.217
	- wt    1   183.347 352.63 82.790
	
	Call:
	lm(formula = mpg ~ wt + qsec + am)
	
	Coefficients:
	(Intercept)           wt         qsec           am
	9.618       -3.917        1.226        2.936
	\end{verbatim}
\end{framed}
\end{document} 
\end{document}
