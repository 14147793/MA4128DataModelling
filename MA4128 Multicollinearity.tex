
\section{Overfitting}
Overfitting occurs when a statistical model does not adequately describe of the underlying relationship between variables in a regression model. Overfitting generally occurs when the model is excessively complex, such as having too many parameters (i.e. predictor variables) relative to the number of observations. A model which has been overfit will generally have poor predictive performance, as it can exaggerate minor fluctuations in the data.

\newpage
\section{Multicollinearity}
In multiple regression, two or more predictor variables are colinear if they show strong linear relationships. This makes estimation of regression coefficients impossible. It can also produce unexpectedly large estimated standard errors for the coefficients of the X variables involved.

This is why an exploratory analysis of the data should be first done to see if any collinearity among explanatory variables exists. Multicolinearity is suggested by non-significant results in individual tests on the regression coefficients for important explanatory (predictor) variables. Multicolinearity may make the determination of the main predictor variable having an effect on the outcome difficult.

\subsection{How to Identify Multicollinearity}


     You can assess multicollinearity by examining \textbf{tolerance} and the \textbf{Variance Inflation Factor} (VIF) are two collinearity diagnostic factors that can help you identify multicollinearity. Tolerance is a measure of collinearity reported by most statistical programs such as SPSS; the variableÂ’s tolerance is 1-R2. A small tolerance value indicates that the variable under consideration is almost a perfect linear combination of the independent variables already in the equation and that it should not be added to the regression equation. All variables involved in the linear relationship will have a small tolerance. Some suggest that a tolerance value less than 0.1 should be investigated further. If a low tolerance value is accompanied by large standard errors and nonsignificance, multicollinearity may be an issue.


\subsection{The Variance Inflation Factor (VIF)}



     The Variance Inflation Factor (VIF) measures the impact of collinearity among the variables in a regression model. The Variance Inflation Factor (VIF) is 1/Tolerance, it is always greater than or equal to 1. There is no formal VIF value for determining presence of multicollinearity. Values of VIF that exceed 10 are often regarded as indicating multicollinearity, but in weaker models values above 2.5 may be a cause for concern. In many statistics programs, the results are shown both as an individual R2 value (distinct from the overall R2 of the model) and a Variance Inflation Factor (VIF). When those R2 and VIF values are high for any of the variables in your model, multicollinearity is probably an issue. When VIF is high there is high multicollinearity and instability of the b and beta coefficients. It is often difficult to sort this out. \\

\bigskip

You can also assess multicollinearity in regression in the following ways:

\begin{itemize}
\item [(1)] Examine the correlations and associations (nominal variables) between independent variables to detect a high level of association. High bivariate correlations are easy to spot by running correlations among your variables. If high bivariate correlations are present, you can delete one of the two variables. However, this may not always be sufficient.

\item [(2)] Regression coefficients will change dramatically according to whether other variables are included or excluded from the model. Play around with this by adding and then removing variables from your regression model.

\item [(3)] The standard errors of the regression coefficients will be large if multicollinearity is an issue.

\item [(4)] Predictor variables with known, strong relationships to the outcome variable will not achieve statistical significance. In this case, neither may contribute significantly to the model after the other one is included. But together they contribute a lot. If you remove both variables from the model, the fit would be much worse. So the overall model fits the data well, but neither X variable makes a significant contribution when it is added to your model last. When this happens, multicollinearity may be present.

\end{itemize}

\subsection{Variance Inflation Factor}

The variance inflation factor (VIF) quantifies the severity of multicollinearity in a regression analysis.

The VIF provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity.


A common rule of thumb is that if the VIF is greater than 5 then multicollinearity is high. Also a VIF level of 10 has been proposed as a cut off value.

\subsection{The adjusted coefficient of determination}
Adjusted $R^2$ (often written as  and pronounced "R bar squared") is a modification of R2 that adjusts for the number of explanatory terms in a model. Unlike $R^2$, the adjusted $R^2$ increases only if the new term improves the model more than would be expected by chance. The adjusted $R^2$ can be negative, and will always be less than or equal to R2.


$R^2_i$ is the unadjusted $R^2$ when you regress $X_i$ against all the other explanatory variables in the model.

Suppose we have four independent variables $X_1$, $X_2$,$X_3$ and $X_4$.
The unadjusted $R^2$ for the second variables $R^2_2$ is computed from the following linear model:

\[ X_2 = \alpha + \beta_a X_1 + \beta_b X_3 + \beta_c X_4 \]

N.B. The dependent variable $Y$ is not used at all in this case.


Suppose there is no linear relation between $X_i$  and the other explanatory
variables in the model. Then $R^2$ will be zero.

The Variance inflation factor for slope estimate $\hat{\beta}_i$ is $ 1 / 1 - R^2_i $.

The Tolerance for slope estimate $\hat{\beta}_i$ is $1 - R^2_i $.